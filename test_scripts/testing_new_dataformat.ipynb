{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing how to read data from new formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "# TODO : split data into val\n",
    "\n",
    "def _tokenize(raw):\n",
    "    tokens = re.findall(r\"[\\w]+\", raw) # finds every word\n",
    "    normalized_tokens = [token.lower() for token in tokens]\n",
    "    return normalized_tokens\n",
    "\n",
    "\n",
    "_s_re = re.compile(\"^F:\")\n",
    "_q_re = re.compile(\"Q:\")\n",
    "_a_re = re.compile(\"A:\")\n",
    "\n",
    "'''\n",
    "called from read_babi_split\n",
    "defines the vocabulary, paragraphs (x input), questions and answers\n",
    "'''\n",
    "def read_babi_files(file_paths):\n",
    "\n",
    "    vocab_set = set()\n",
    "    paragraphs = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as fh:\n",
    "            lines = fh.readlines()\n",
    "            paragraph = []\n",
    "            for line_num, line in enumerate(lines):\n",
    "                line = line.strip('\\n')\n",
    "                sm = _s_re.match(line) # matches pattern of a sentence\n",
    "                qm = _q_re.match(line) # matches pattern of a question\n",
    "                am = _a_re.match(line)\n",
    "\n",
    "                # if it is a question, peel off the 'Q: ' beginning part\n",
    "                if qm:\n",
    "                    raw_question = line[2:].strip() # should start from the space after Q:\n",
    "                    question = _tokenize(raw_question)\n",
    "                    questions.append(question)\n",
    "                    vocab_set |= set(question)\n",
    "                    # now that we've hit a question we know we're at the end of the \"story\"\n",
    "                    # add the paragraph so far to the paragraphs list\n",
    "                    paragraphs.append(paragraph)\n",
    "                    paragraph = [] # clear the paragraph to start adding new things to it on the next time we hit a sentence\n",
    "                # if it is a sentence/part of the paragraph, peel off the 'A: ' part\n",
    "                elif sm:\n",
    "                    raw_sentence = line[2:].strip()\n",
    "                    sentence = _tokenize(raw_sentence)\n",
    "                    paragraph.append(sentence)\n",
    "                    vocab_set |= set(sentence)\n",
    "                elif am:\n",
    "                    answer = line[2:].strip()\n",
    "                    answers.append(answer)\n",
    "                    vocab_set.add(answer)\n",
    "                else:\n",
    "                    logging.error(\"Invalid line encountered: line %d in %s\" % (line_num + 1, file_path))\n",
    "            \n",
    "            print(\"Loaded %d examples from: %s\" % (len(paragraphs), os.path.basename(file_path)))\n",
    "            \n",
    "    return vocab_set, paragraphs, questions, answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 examples from: custom_train.txt\n"
     ]
    }
   ],
   "source": [
    "vocab_set, paragraphs, questions, answers = read_babi_files([\"/Users/williamcosby/Documents/metis/Passion_Project_Stratus/data/custom/custom_train.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'black',\n",
       " 'blue',\n",
       " 'car',\n",
       " 'color',\n",
       " 'does',\n",
       " 'engine',\n",
       " 'green',\n",
       " 'has',\n",
       " 'have',\n",
       " 'is',\n",
       " 'red',\n",
       " 'the',\n",
       " 'v12',\n",
       " 'v4',\n",
       " 'v6',\n",
       " 'v8',\n",
       " 'what',\n",
       " 'white'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
